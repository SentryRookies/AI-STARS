from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
import atexit
import os
import pandas as pd

from app.database import SessionLocal
from app.model import Review_summarize

from batch.emotion_model import analyze_reviews
from batch.keyword_model import extract_top_keywords

def crawl_and_analyze():
    print("ğŸ“¦ [ìŠ¤ì¼€ì¤„ëŸ¬] CSV ì½ê¸° + ë¶„ì„ ì‹œì‘")

    db = SessionLocal()
    data_dir = "./data"

    try:
        for file_name in os.listdir(data_dir):
            if file_name.endswith(".csv"):
                file_path = os.path.join(data_dir, file_name)
                print(f"ğŸ“‚ íŒŒì¼ ë¶„ì„ ì‹œì‘: {file_path}")

                # âœ… target_type, target_id ì¶”ì¶œ
                name_parts = file_name.replace(".csv", "").split("_")
                if len(name_parts) < 3:
                    print(f"âš ï¸ íŒŒì¼ ì´ë¦„ í˜•ì‹ ì˜¤ë¥˜: {file_name}")
                    continue

                target_type = name_parts[0]
                target_id = name_parts[1]

                df = pd.read_csv(file_path, encoding="utf-8-sig")
                df = df.dropna(subset=["content"])
                reviews = [{"content": text} for text in df["content"]]

                validated_data = analyze_reviews(reviews)
                validated_data = [r for r in validated_data if r and all(k in r for k in ("text", "label", "score"))]

                print(f"âœ… ê²€ì¦ëœ ë°ì´í„° ìˆ˜: {len(validated_data)}")
                analyzed_df = pd.DataFrame(validated_data)

                pos_keywords, neg_keywords = extract_top_keywords(validated_data)

                pos_count = analyzed_df[analyzed_df["label"] == "positive"].shape[0]
                neg_count = analyzed_df[analyzed_df["label"] == "negative"].shape[0]

                # âœ… content ë¬¸ìì—´ë¡œ í†µí•©
                content_summary = (
                    f"[ê¸ì • í‚¤ì›Œë“œ] {', '.join(pos_keywords)}\n"
                    f"[ë¶€ì • í‚¤ì›Œë“œ] {', '.join(neg_keywords)}\n"
                    f"[ê¸ì • ë¼ë²¨ ìˆ˜] {pos_count}\n"
                    f"[ë¶€ì • ë¼ë²¨ ìˆ˜] {neg_count}"
                )

                summary = Review_summarize(
                    target_id=target_id,
                    target_type=target_type,
                    content=content_summary
                )
                db.add(summary)
                db.commit()
                print(f"âœ… {file_name} ë¶„ì„ ë° ì €ì¥ ì™„ë£Œ")

    except Exception as e:
        db.rollback()
        print("âŒ ì˜¤ë¥˜ ë°œìƒ:", e)
    finally:
        db.close()

def start_scheduler():
    scheduler = BackgroundScheduler()

    # ë§¤ë‹¬ 1ì¼ ì˜¤ì „ 3ì‹œ
    scheduler.add_job(crawl_and_analyze, CronTrigger(day=1, hour=3, minute=0))

    scheduler.start()
    print("ğŸ•’ APScheduler ì‹œì‘ë¨")

    atexit.register(lambda: scheduler.shutdown())

# (ë©”ì¸ ì„œë²„ íŒŒì¼ì—ì„œ ì‹¤í–‰ìš©)
if __name__ == "__main__":
    start_scheduler()
