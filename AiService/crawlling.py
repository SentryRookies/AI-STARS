import asyncio
from playwright.async_api import async_playwright
import csv
import datetime
import os
from sqlalchemy import create_engine, text

# DB ì—°ê²° ì •ë³´
DB_URL = "postgresql://postgres:example@k8s-default-postgres-2cdafece7a-ac1876b9c7d705f9.elb.ap-northeast-2.amazonaws.com:5432/stars_db"
# tables = ["accommodation", "attraction", "cafe", "restaurant"]
tables = ["accommodation"]
MAX_REVIEWS = 100
MIN_LENGTH = 10

# rowì—ì„œ id ì¶”ì¶œ
def get_row_id(row, table_name):
    return getattr(row, f"{table_name}_id")

# í¬ë¡¤ë§ ëŒ€ìƒ ê°€ì ¸ì˜¤ê¸°
def load_targets(resume_from_id=None):
    crawl_targets = []
    engine = create_engine(DB_URL)
    with engine.connect() as connection:
        for table in tables:
            result = connection.execute(text(f"SELECT {table}_id, kakaomap_url FROM {table}"))
            for row in result:
                if not row.kakaomap_url:
                    continue
                crawl_targets.append({
                    "table": table,
                    "id": get_row_id(row, table),
                    "kakaomap_url": row.kakaomap_url
                })
    if resume_from_id:
        for i, item in enumerate(crawl_targets):
            if item["table"] == "accommodation" and item["id"] == resume_from_id:
                print(f"ğŸ” ID {resume_from_id} ì´í›„ë¶€í„° ì¬ì‹œì‘í•©ë‹ˆë‹¤ (index {i+1}ë¶€í„°)")
                return crawl_targets[i + 1:]  # í•´ë‹¹ ID ë‹¤ìŒ ì¸ë±ìŠ¤ë¶€í„° ë°˜í™˜
    return crawl_targets

# í¬ë¡¤ëŸ¬ ì‹¤í–‰ (urlë³„)
async def run_crawler(url, table, item_id):
    now = datetime.datetime.now()
    results = []
    save_dir = "./data"
    os.makedirs(save_dir, exist_ok=True)
    file_name = f"{table}_{item_id}_{now.strftime('%Y%m%d_%H%M')}.csv"
    file_path = os.path.join(save_dir, file_name)

    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            await page.goto(url)
            await page.wait_for_timeout(3000)

            # ì •ë ¬ í´ë¦­
            try:
                await page.click("button.btn_sort")
                await page.wait_for_timeout(100)
                await page.click("a.link_sort")
            except:
                pass

            # ìŠ¤í¬ë¡¤
            for _ in range(10):
                await page.evaluate("window.scrollBy(0, document.body.scrollHeight);")
                await page.wait_for_timeout(1000)

            # ë”ë³´ê¸° ë²„íŠ¼
            more_buttons = await page.query_selector_all("span.btn_more")
            for button in more_buttons:
                try:
                    if "ë”ë³´ê¸°" in await button.inner_text():
                        await button.click()
                        await page.wait_for_timeout(500)
                except:
                    continue

            # ë¦¬ë·° ìˆ˜ì§‘
            review_elements = await page.query_selector_all("p.desc_review")
            count = 0
            for review in review_elements:
                text = (await review.inner_text()).strip().replace("\n", " ")
                if len(text) >= MIN_LENGTH:
                    results.append({"content": text})
                    count += 1
                if count >= MAX_REVIEWS:
                    break

            await browser.close()

        # ì €ì¥
        if len(results) >= 10:
            with open(file_path, mode="w", newline="", encoding="utf-8-sig") as f:
                writer = csv.DictWriter(f, fieldnames=["content"])
                writer.writeheader()
                writer.writerows(results)
            print(f"âœ… [{table} - {item_id}] {len(results)}ê°œ ë¦¬ë·° ì €ì¥ ì™„ë£Œ: {file_path}")
        else:
            print(f"âš ï¸  [{table} - {item_id}] ë¦¬ë·° ê°œìˆ˜ {len(results)}ê°œë¡œ ì €ì¥í•˜ì§€ ì•ŠìŒ")

    except Exception as e:
        print(f"âŒ [{table} - {item_id}] ì—ëŸ¬ ë°œìƒ: {e}")

# ì „ì²´ ì‹¤í–‰
async def main():
    # targets = load_targets()
    # ì—¬ê¸°ì„œ ì›í•˜ëŠ” ê¸°ì¤€ ID ë„£ê¸°
    targets = load_targets(resume_from_id=391589894)

    for target in targets:
        url_with_comment = target["kakaomap_url"] + "#comment"
        await run_crawler(
            url=url_with_comment,
            table=target["table"],
            item_id=target["id"]
        )
        await asyncio.sleep(2)  # ë„ˆë¬´ ë¹ ë¥´ë©´ ì°¨ë‹¨ë  ìˆ˜ ìˆìœ¼ë‹ˆ ì ê¹ ì‰¬ê¸°

if __name__ == "__main__":
    asyncio.run(main())
